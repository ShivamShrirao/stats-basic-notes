{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540d1b70-24dd-4b33-b3aa-766da89fe88d",
   "metadata": {},
   "source": [
    "# Stats Basics\n",
    "> Just some basic stats concepts for quick revision.\n",
    "\n",
    "- toc: true \n",
    "- comments: true\n",
    "- hide: false\n",
    "- image: images/stats/high_bias.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a804d0a-2ea7-48a9-899a-64cc3403df5d",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "- Split the training data into multiple folds, eg. Four folds of 25% each.\n",
    "- For each fold as testing, and rest as training train a model each time.\n",
    "- Do same with different models. Choose the one which gets the most folds correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4a49d-9287-45fc-bf33-34615a58843e",
   "metadata": {},
   "source": [
    "# Sensitivity & Specificity\n",
    "\n",
    "- **Sensitivity**: Percentage of positives correctly identified. (True Positive rate)\n",
    "    - Eg.: Percentage of patients correctly identified to have a disease among all actually having disease.\n",
    "    - Sensitivity = $\\frac{TP}{TP+FN}$\n",
    "\n",
    "\n",
    "- **Specificity**: Percentage of negatives correctly identified. (True Negative rate)\n",
    "    - Eg.: Percentage of patients correctly identified to NOT have a disease among all actually NOT having disease.\n",
    "    - Specificity = $\\frac{TN}{TN+FP}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5560d58-56ea-4852-9f2d-ff69a1811b2d",
   "metadata": {},
   "source": [
    "|                   | Real Has | Real Not Has |\n",
    "|-------------------|----------|--------------|\n",
    "| Predicted Has     |    TP    |      FP      |\n",
    "| Predicted Not Has | FN       | TN           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd13584e-708c-44d5-bcc7-61c88e42aef0",
   "metadata": {},
   "source": [
    "# Precision & Recall\n",
    "\n",
    "- **Precision**: Proportion of positives correctly identified.\n",
    "    - Eg.: Number of patients actually having the disease among all prediicted to be having the disease.\n",
    "    - Precision = $\\frac{TP}{TP+FP}$\n",
    "\n",
    "\n",
    "- **Recall** = [Sensitivity](#Sensitivity-&-Specificity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5e532-847e-437b-8f80-16dd96eee009",
   "metadata": {},
   "source": [
    "# Bias & Variance\n",
    "\n",
    "- **Bias**: \"Prior assumptions\" in model/algorithm preventing it to fit on data.\n",
    "    - Eg: Linear regression will be a straight line so can't fit on curved data, therefore it has high bias and low variance.\n",
    "    ![](images/stats/high_bias.png \"https://youtu.be/EuBBz3bI-aA\")  \n",
    "        *Image Credit: [Machine Learning Fundamentals: Bias and Variance](https://youtu.be/EuBBz3bI-aA)*\n",
    "\n",
    "\n",
    "- **Variance**: Model/algorithm is can \"vary\" itself alot and thus can overfit on data.\n",
    "    - Eg: Lot of curves going exactly through all data points, model won't generalize, therefore it has high variance and low bias.\n",
    "    ![](images/stats/high_variance.png \"https://youtu.be/EuBBz3bI-aA\")  \n",
    "        *Image Credit: [Machine Learning Fundamentals: Bias and Variance](https://youtu.be/EuBBz3bI-aA)*\n",
    "\n",
    "\n",
    "- **Bias Variance Tradeoff**: We gotta find a good comprise among the two to find best model. \n",
    "\n",
    "\n",
    "**Intersting Fact**: Check out double descent in deep learning, highly over-parameterized models can at first overfit, perform worse but then start to perform better again.  \n",
    "An intuitive possible explanation: https://twitter.com/daniela_witten/status/1292293102103748609"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c9512-8212-452d-b150-32abdabe7de3",
   "metadata": {},
   "source": [
    "# ROC & AUC\n",
    "\n",
    "- __Receiver Operator Characterstic (ROC)__: Plot True Positive Rate vs False Positive Rate of model for different classification thresholds.  \n",
    "    - Decide what works better for you, if you want classify positives more correctly and some false positives,\n",
    "    - OR lesser true positives for no false positives.  \n",
    "    ![](images/stats/roc.png \"https://youtu.be/4jRBRDbJemM\")  \n",
    "        *Image Credit: [ROC and AUC, Clearly Explained!](https://youtu.be/4jRBRDbJemM)*\n",
    "\n",
    "\n",
    "- __Area Under Curve (AUC)__: Compare ROC graphs.  \n",
    "    - Graph with higher area better.  \n",
    "    - False positive rate can often be replaced with Precision.  \n",
    "    ![](images/stats/auc.png \"https://youtu.be/4jRBRDbJemM\")  \n",
    "        *Image Credit: [ROC and AUC, Clearly Explained!](https://youtu.be/4jRBRDbJemM)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10da93-6621-4616-a5f4-d54c44575768",
   "metadata": {},
   "source": [
    "# Residuals\n",
    "- __Residuals__ : Vertical distance of data point from line.\n",
    "- __R squared__ : $\\large1 -\\frac{\\text{Sum of squares of Residuals}}{\\text{Total sum of squares(variance)}}$.  \n",
    "0 -> Worst, 1 -> Best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b4918-48b6-411d-ab4c-c1803a275fbe",
   "metadata": {},
   "source": [
    "$log(odds) = log(\\frac{p}{1-p})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb6628-df36-45fe-a2b9-bedd2ad2146e",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Probability between 0 and 1.\n",
    "Convert to log(odds) on y axis to compare to linear regression.\n",
    "\n",
    "- __Uses Maximum Likelihood__: Log adds for datapoints converted to probability.  \n",
    "- __Log Likelihood__: Sum of logs of probabilities or multiplication of probabilities.  \n",
    "![](images/stats/logistic_likelihood.png \"https://youtu.be/BfKanl1aSG0\")  \n",
    "    *Image Credit: [Logistic Regression Details Pt 2: Maximum Likelihood](https://youtu.be/BfKanl1aSG0)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24540ad9-2440-4235-b15c-9f6357d953eb",
   "metadata": {},
   "source": [
    "# Ridge (L2) and Lasso (L1) Regression\n",
    "\n",
    "- __Ridge (L2)__: $\\text{sum of the squared residuals} + \\lambda\\times\\text{slope}^2$ &nbsp;&nbsp;&nbsp;&nbsp; <-- Penalty\n",
    "- __Lasso (L1)__: $\\text{sum of the squared residuals} + \\lambda\\times\\text{|slope|}$ &nbsp;&nbsp;&nbsp;&nbsp; <-- Penalty\n",
    "- can have all different parameters except y intercept in place of slope.\n",
    "- makes line less senitive to changes in weight, decreases slope.\n",
    "- gets line with little bit of bias but less variance.\n",
    "- higher lambda -> lower slope\n",
    "- parameters can be shrunk all the way to zero in Lasso(L1) unlike Ridge(L2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59b95a3-4f72-413b-999d-def8bcbf7793",
   "metadata": {},
   "source": [
    "# T-SNE\n",
    "- Find scaled similarity scores among all points.\n",
    "- Randomly project on linear line.\n",
    "- Calculate new similarity scores.\n",
    "- Iteratively make similar points be together and others further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363cbedc-ed7d-4f64-9303-1437436f467e",
   "metadata": {},
   "source": [
    "# Hierarchial clustering\n",
    "- Put similar rows together.\n",
    "- Consider them as 1 row.\n",
    "- Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679324d-d46d-40dc-960e-b39dd7901bc9",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "- Calculate total impurity (gini, entropy, etc.) for each feature. And select least impure as root node.\n",
    "- Repeat for branches.\n",
    "- In Regression trees, find threshold to split the data having least sum of squared residuals.\n",
    "- Tree score = sum of squared residuals $+\\ \\alpha\\ \\times$ number of leaves(T)\n",
    "\n",
    "## Random Forest\n",
    "- Random forest uses Bagging. Bootstrap and aggregating.\n",
    "- Take subset of samples, take subset of features. Make DT.\n",
    "- All DTs have equal vote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47641ff-5577-4e8a-b008-954a06c4bafe",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "- Combines weak learners (stumps - uses single feature) for classification.\n",
    "- Some stumps get more say in classification than others. (as compared to Random forest with all equal.)\n",
    "- Each stump takes into account mistakes of previous stump.\n",
    "    - Choose first stump with least gini index. It's say depends on how well it classifies.\n",
    "    - amount of say = $\\frac{1}{2}log\\left(\\frac{1-\\text{Total Error}}{\\text{Total Error}}\\right)$\n",
    "    - Increase the sample weight of incorrectly classified sample = sample weight $\\times\\ e^{\\text{amount of say}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477eefe-8dff-45d1-9305-53f0154f43f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
